import numpy as np
import scipy.io
import csv
import re

np.random.seed(123)

class Graph:
    def __init__(self, filename, source_type="adjacency matrix", directed=False, weighted=False, memory_control=False):
        '''read in the original representation of network and convert it to other types of
        representations'''

        self.directed = directed
        self.weighted = weighted
        self.mem_ctl = memory_control
        
        with open(filename, newline='') as f:
            data_reader = csv.reader(f, delimiter=',', quotechar='|')
            graph = np.array([row for row in data_reader]).astype(float) # Needs to be improved to
                                                                       # handle string data
        # If necessary, will map strings to ids, id starts with 0
        # This part is left for future development
        #
        if source_type == "adjacency matrix":
            self.n = graph.shape[0] # node number
            self.adj_mat = graph 

            self.adj_lst = [] # adjacency list
            self.m = 0
            for v in range(self.n):
                nghbrs = [int(i) for i, w in enumerate(graph[v]) if w !=0]
                self.m += len(nghbrs)
                ws = graph[v][nghbrs].tolist()
                self.adj_lst.append([nghbrs, ws])

            self.edge_lst = [] # edge list 

        elif source_type == "edge list":
            # e.g.
            # 1 2 0.5
            # 1 3 1.0
            # 2 3 1.5
            # where the first column is the source node, second the target node,
            # and the third, the weight between source and target nodes. The third
            # column may not exist.
            # if the graph is undirected, say v1 and v2 are neighbors
            # the original edge list must have records of both (v1, v2) and (v2, v1)
            # also, the program requires the node id starts with 0
            self.m = graph.shape[0] 
            # if no weights information, treat weight as 1.0
            if graph.shape[1]==2: graph = np.column_stack((graph, np.ones(self.m)))
            #self.edge_lst = graph.tolist() # node type is float! be careful!
            self.edge_lst = []

            graph = graph[graph[:, 0].argsort()]
            self.n = int(max(graph[-1][0], np.max(graph[:, 1]))) + 1
            if memory_control == False: self.adj_mat = np.zeros((self.n, self.n))

            self.adj_lst = []
            v = graph[0][0]
            nghbrs = []
            ws = []
            for e in range(self.m):
                v_src = int(graph[e][0]) # source node
                nghbr = int(graph[e][1])  # target node
                w = graph[e][2]     # weight
                if memory_control == False: self.adj_mat[v_src][nghbr] = w
                if v_src == v:
                    nghbrs.append(nghbr)
                    ws.append(w)
                else:
                    self.adj_lst.append([nghbrs, ws])
                    v = v_src
                    nghbrs = [nghbr]
                    ws = [w]
            self.adj_lst.append([nghbrs, ws])

        if directed==False: self.m = int(self.m/2)
        if memory_control == False:
        	self.degree = np.sum(self.adj_mat, axis=1)
        else:
        	self.degree = np.array([len(self.adj_lst[v][0]) for v in range(self.n)])
        self.tol_degree = np.sum(self.degree) # summation of degrees

        self.nodes_set = set(range(self.n))
        self.st_times = np.zeros(self.n).astype(int) # times nodes being starting points
        #self.node_id = [] # Handle the case where nodes are not numbers but strings
        #self.id_node = [] # This feature can be added in the future


    def reorder_community(self, community):
        '''read in communities generated by RW_HDP or SIP_LDA and then reorder communities
        param community: a vector of length V, recording the community of each node, e.g.
        [0, 2, 1, 4, 3, 0, 8, 0]. the ids of communities are not necessarily continuous,
        which is why we need to reorder their ids to make them continuous'''

        coms = np.sort(np.unique(community))
        self.num_coms = K = len(coms) # number of communities
        coms_ = {}
        for new_id, com in enumerate(coms):
            coms_[com] = new_id

        S = np.zeros((self.n, K), dtype=np.float64)
        for v in range(self.n):
            r = coms_[community[v]]
            S[v][r] = 1.0
        self.node_community = S # matrix representation of communities, shape (V, K)
        
        self.communities = [] # store members of each community
        for k in range(K):
            self.communities.append(list(np.flatnonzero(S[:, k])))


    def read_community(self, filename, algo='BCD'):
        '''
        read communities generated by other partition algorithms: BCD and Walktrap
        '''
        if algo == 'BCD':
            mat_structure = scipy.io.loadmat(filename)
            self.node_community = mat_structure['Z'].T

            self.num_coms = K = self.node_community.shape[1] # number of communities
            self.communities = []
            for k in range(K):
                self.communities.append(list(np.flatnonzero(self.node_community[:, k])))

        elif algo == 'Walktrap':
            with open(filename) as f:
                content = f.readlines()
            content = content[1:] 

            self.num_coms = len(content)
            self.communities = []
            self.node_community = np.zeros((self.n, self.num_coms))
            
            for i in range(self.num_coms):
                tmp = re.split(r'[;,\s\{\}]\s*', content[i])
                node_pos = [id for id, ele in enumerate(tmp) if ele.isdigit()][1:]

                members = []
                for j in range(len(node_pos)):
                    node = int(tmp[node_pos[j]])
                    self.node_community[node][i] = 1
                    members.append(node)
                self.communities.append(members)

            
    def get_modularity(self):
        '''see Wikipedia: network modularity for calculation details'''
        if self.mem_ctl == False:
        	l = np.sum(self.degree)
        	B = self.adj_mat - np.outer(self.degree, self.degree)/l
        	self.modularity = np.trace(self.node_community.T.dot(B).dot(self.node_community)) / l
        else:
        	K = len(self.communities)
        	e = np.zeros((K, K))
        	if self.directed == False and self.weighted == False:
        		for i in range(K):
        			com_i = set(self.communities[i])
        			for j in range(i, K):
        				tmp = 0.0
        				com_j = set(self.communities[j])
        				for v in com_i:
        					tmp += len(set(self.adj_lst[v][0]).intersection(com_j))
        				e[i][j] = e[j][i] = tmp / self.tol_degree

        		a = np.power(np.sum(e, axis=1), 2)
        		self.modularity = np.sum([e[i][i] - a[i] for i in range(K)])
        	else:
        		pass

    def get_conductance(self):
        '''calculate conductance of each community'''
        self.conductance = []
        for k in range(self.num_coms):
            members = self.communities[k] # members of the community
            v = np.sum([self.degree[members[i]] for i in range(len(members))]).astype(float)
            e = np.empty(len(members))
            for i in range(len(members)):
                node = members[i]
                nghbrs = self.adj_lst[node][0] # all neighbors of the node
                com_nghbrs = list(set(members).intersection(nghbrs)) # neighbors within the same community
                #e[i] = np.sum(self.adj_mat[node, com_nghbrs])
                e[i] = len(com_nghbrs)
            self.conductance.append((v-np.sum(e))/v)


    def get_partition_intersections(self, ref_partition):
        '''
        param ref_partition: the structure of this variable is the same as that of self.communities
        we have two partitions: self.communities, generated by our partition algorithm, and 
        ref_partition, the true partition. Denote self.communities by X = {X1, X2, ..., Xn}, and
        ref_partition by Y = {Y1, Y2, ..., Ym}. we want to find nij = card(Xi intersect Yj)'''
        self.ref_partition = ref_partition
        n = len(self.communities)
        m = len(ref_partition)

        self.com_len = np.array([len(self.communities[i]) for i in range(n)])
        self.ref_par_len = np.array([len(ref_partition) for j in range(m)])
        self.partition_intersections = np.empty((n, m)).astype(float)
        for i in range(n):
            for j in range(m):
                self.partition_intersections[i][j] = len(set(self.communities[i]).intersection(set(ref_partition[j])))

    def get_F_measure(self):
        '''calculate F meaure, see paper: Generalized Measures for the Evaluation of
        Community Detection Methods
        '''
        (n, m) = self.partition_intersections.shape
        self.purity = np.sum([np.max(self.partition_intersections[i]) for i in range(n)]) / self.n
        self.inverse_purity = np.sum([np.max(self.partition_intersections[:, j]) for j in range(m)]) / self.n
        self.F_measure = 2*self.purity*self.inverse_purity / (self.purity + self.inverse_purity)        


    def get_ARI(self):
        '''calculate the Adjusted Rand Index, for details, see the same paper as in get_F_measure()'''
        M = np.sum(self.partition_intersections*(self.partition_intersections-1)) / 2

        X_ = np.sum(self.com_len*(self.com_len-1)) / 2
        Y_ = np.sum(self.ref_par_len*(self.ref_par_len-1)) / 2
        E_M = X_ * Y_ / (self.n*(self.n-1)/2)
        M_max = (X_ + Y_) / 2

        self.ARI = (M-E_M) / (M_max - E_M)


    def get_NMI(self):
        '''calculate Normalized Mutual Information, for details, see the same paper as in get_F_measure()'''
        (n, m) = self.partition_intersections.shape

        P = self.partition_intersections / self.n
        P_X = np.sum(P, axis=1)
        P_Y = np.sum(P, axis=0)

        # mutual information
        tmp = P / (np.diag(P_X).dot(np.ones((n, m))).dot(np.diag(P_Y)))
        tmp[tmp==0] = 1.0 # Pij is zero if nij is zero, thus we can have zero elements in tmp
                          # to avoid log(0), we replace zero elements with 1.0. This won't
                          # affect the correctness of calculating mutual information I, because
                          # Pij * log (tmpij) = Pij * log(1.0) = 0, when Pij is zero
        I = np.sum(P * np.log(tmp))
        # entropy
        H_X = np.sum(P_X * np.log(P_X))
        H_Y = np.sum(P_Y * np.log(P_Y))
        # normalized mutual information
        self.NMI = -2*I / (H_X + H_Y)


    def get_density(self):
        '''calculate density of each community'''
        self.density = []
        for k in range(self.num_coms):
            members = set(self.communities[k]) # members of the community
            S = len(members)
            E = 0.0
            for v in members:
                nghbrs = self.adj_lst[v][0]
                com_nghbrs = members.intersection(nghbrs)
                E += len(com_nghbrs)
            if S-1 == 0:
                self.density.append(0)
            else:
                self.density.append(E/(S*(S-1)))


    def get_cut_ratio(self):
        '''calculate cut ratio of each community'''
        self.cut_ratio = []
        for k in range(self.num_coms):
            members = self.communities[k] # members of the community
            S = len(members)
            C_s = 0.0
            for v in members:
                nghbrs = set(self.adj_lst[v][0])
                out_nghbrs = nghbrs.difference(members)
                C_s += len(out_nghbrs)
            self.cut_ratio.append(C_s/(S*(self.n-S)))


    def get_TPR(self):
        '''calculate triangle participation rate'''
        self.TPR = []
        for k in range(self.num_coms):
            members = self.communities[k] # members of the community
            num_triads = 0.0 # number of nodes in the community that is the center of at least one tirad that 
                             # belongs to the community
            for u in members: # center of the triad
                u_com_nghbrs = set(self.adj_lst[u][0]).intersection(members) # neighbors of node u in the community
                for v in u_com_nghbrs:
                    v_com_nghbrs = set(self.adj_lst[v][0]).intersection(members) # neighbors of node v in the community
                    if len(v_com_nghbrs.intersection(u_com_nghbrs)) > 0:
                        num_triads += 1
                        break
            self.TPR.append(num_triads/len(members))
